% %Open Data
% Recent times have seen the deployment of the Open Data initiative due to the strategy 
% developed by the governments in USA, Australia, UK or Europe and it has been followed by most of countries around the world to deliver data 
% catalogues containing valuable public sector information (PSI). In this context the Open (Government) Data movement (W3C) is making a great effort 
% to spread this view in public and private bodies with the objective of boosting transparency and a new data-driven economy. 
% From a corporate strategy point of view the re-use of existing open data must encourage and improve 
% more efficient policy-making processes. Under this view there is a perfect matching between the Linked Data and the Open Data 
% principles: on the one hand the Linked Data community is generating the proper environment of standard 
% specifications and tools to manage data and, on the other hand, the Open Data initiative is requiring 
% the right methods to generate an authentic re-use of public data. The combination of these 
% two views leads us to the Linked Open Data (LOD) effort that seeks for applying the principles of Linked Data to implement the Open Data mission.
% 
Currently one of the mainstreams in the Semantic Web area lies in the application of the Linked Open Data initiative in 
different domains such as  e-Government, e-Procurement, e-Health or Biomedicine to name a few,  
with the aim of solving existing problems of integration and interoperability among applications and create a 
knowledge environment under the Web-based protocols. In this context, the present work is therefore focused 
in applying semantic web vocabularies and datasets to model quantitative indexes from both structural 
and computational points of view in a ``Quality Management'' context. In order to reach the major objective of building a re-usable data environment,  
the publication of information and data under a common data model (RDF) and formats with 
a specific formal query language (SPARQL), such as the OSLC initiative aims, provides the required building blocks to turn 
a document-centric realm into a real database of data. Research works are focused in two main areas: 1) data production/publishing, 
with special focus on quality~\cite{Bizer:2009:QIF:1482178.1482280}, conformance, provenance and trust~\cite{w3c-prov}, description of datasets with the Vocabulary of Interlinked Datasets (VoID) 
and Data Catalog Vocabulary (DCAT) or entity reconciliation~\cite{Maali_Cyganiak_2011} processes and 2) data consumption. More specifically, visualization~\cite{DBLP:journals/semweb/DadzieR11}, 
faceted browsing and searching~\cite{hoga-etal-2011-swse-JWS}, distributed queries, scalable reasoning processes or annotation of 
web pages~\cite{rdfa-primer} among others are key research topics to deploy the Semantic Web and the Linked Data initiative.

% In the first case data quality~\cite{wiqa}, conformance~\cite{HoganUHCPD:2012:237}, 
% provenance, trust~\cite{Carroll05namedgraphs}, description of datasets and 
% entity reconciliation are becoming major objectives since a mass of amount data is already available 
% through SPARQL endpoints deployed on the top of RDF repositories. On the other hand, consumption of Linked Data is being addressed 
% to provide new ways of data visualization, faceted browsing and searching, 
% exploitation of data applying different approaches such as sensors~\cite{Jeung:2010:EMM:1850003.1850235} and techniques such as distributed 
% queries\cite{Hartig09executingsparql}, scalable reasoning process or 
% annotation of web pages to name a few.

In the particular case of statistical data, the RDF Data Cube Vocabulary~\cite{rdf-data-cube}
a W3C Working Draft document, is a shared effort to represent statistical data in RDF reusing parts (the cube model) 
of the Statistical Data and Metadata Exchange Vocabulary (SDMX)~\cite{sdmx}, an ISO standard 
for exchanging and sharing statistical data and meta-data among organizations. The Data Cube vocabulary is a core 
foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or 
other multi-dimensional data sets. Previously, the Statistical Core Vocabulary~\cite{scovo} was the standard in fact to describe statistical information in the Web of Data.
Some works are also emerging to mainly publish statistical data following the concepts of the LOD initiative 
such as~\cite{DBLP:conf/semweb/ZapilkoM11,DBLP:journals/ijsc/SalasMBCMA12,DDI2013,DBLP:conf/dgo/FernandezMG11,webindexlod} 
among others.

All the aforementioned works must be considered in order to re-use existing vocabularies and datasets to address 
the challenges of creating meta-described quantitative indexes. Mainly semantics allows us to model logical restrictions 
on data and the computation process while linked data enables the description of indexes in terms of existing concepts and 
the publication of new data and information under a set of principles to boost their re-use and automatic 
processing through machine-readable formats and access protocols.


In case of systems and software engineering, the continuous reviewing of requirements quality and other software 
metrics~\cite{DBLP:journals/sqj/MontagudAI12} ensure the fulfilling of stakeholder's operational capabilities and 
the correction or elimination of obsolete requirements that can impact in future enhancements. To do so requirements 
engineering (RE) methods~\cite{DBLP:journals/is/CastroKM02} must be available to trace from start to finish any design. 
Thus the initial stakeholder's intentions and user needs can be easily verified and validated against any existing or 
baseline model~\cite{DBLP:journals/jss/ZhengAO08}. In this sensse, the success of software development 
process~\cite{DBLP:journals/re/Ralph13} is usually related to the right understanding and quality of requirements. Thus the relevance 
of requirements and, therefore, their quality can be seen as a metric to evaluate whether a project is success or 
not. That is why a good number of tools and works can be found~\cite{DBLP:conf/refsq/2013} to: 1) measure requirements 
quality~\cite{DBLP:journals/re/GenovaFMHM13}; 2) model quality functions~\cite{DBLP:journals/ijseke/SaekiHK13} or 3) implement 
existing ISO standards~\cite{DBLP:journals/sqj/CoteSLM05}. Although these points have been successfully addressed the 
necessity of exposing data quality through an interoperable such as OSLC requires the use of semantic-based 
technologies such as RDF or SPARQL. In this sense, previous efforts in quality management of requirements can 
be re-used but both structure and computation must be now offered in a target interoperable data-based environment.



