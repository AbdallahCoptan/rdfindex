Data filtering, quality and aggregation is a major challenge in the 
new data-driven economy. The interest of public and private bodies 
to manage and exploit information is growing and specific activities 
such as policy making must take advantage of compiling data and information 
to make more timely and accurate decisions. In this sense the use 
of quantitative indexes has been widely accepted as a practice to 
summarize diverse key performance indicators and establish a numerical 
value with the aim of comparing different alternatives. Nevertheless traditional techniques 
and the ``infobesity'' are preventing a proper use of information and the dynamism 
of this new environment also requires a high-level of integration and interoperability. In this 
context the Semantic Web area and, more specifically the Linked Data initiative, 
provide a standardized stack of technologies for knowledge-based techniques with 
the aim of boosting data management and exploitation. That is why the application 
of semantic web principles to model quantitative indexes from both structural and 
computational points of view can fulfill the requirements of this new data-based 
environment and leverage semantics to create a real knowledge-based society. The 
present work has introduced two main contributions: 1) the RDFIndex vocabulary to model and compute quantitative indexes and 
2) a Java-SPARQL based processor of the RDFIndex to validate, generate and populate new observations.

Although this first effort has applied semantics to a specific problem new enhancements 
must be done to cover all potential requirements in the construction of quantitative indexes. In this sense, the presented 
approach is now being used in the modeling of indexes such as the Cloud Index~\footnote{\url{http://cloudindex-doc.herokuapp.com}} or 
the Web Index~\cite{webindexlod}. Furthermore, the proposed representation contains a very interesting property 
that lies in the possibility of computing indexes and components in a parallel fashion. 
Other issues such as SPARQL-based validation, provenance and trust, real-time updates, storage or visualization are 
also on-going work that must be addressed to fit to new data environments.





