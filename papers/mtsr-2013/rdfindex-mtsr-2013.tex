\documentclass{llncs}

%\usepackage{llncsdoc}

%\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multirow}

\usepackage{url}
\usepackage{rotating}

%%%Math
\usepackage{latexsym}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{eurosans}

\usepackage{eurosym}

\usepackage{longtable}

\usepackage{listings}

\usepackage{color}
\usepackage{textcomp}




\definecolor{gray}{gray}{0.5}
\definecolor{green}{rgb}{0,0.5,0}


\begin{document}

\title{Leveraging semantics to represent and compute quantitative indexes. \\ The RDFindex approach.}

\author{Jose Mar\'{i}a \'{A}lvarez\inst{1} \and Jos\'{e} Emilio Labra\inst{2}  \and Patricia Ordoñez de Pablos\inst{3}}


\authorrunning{Jose Mar\'{i}a Alvarez \and  Jos\'{e} Emilio Labra \and Patricia Ordoñez de Pablos}


\institute{South East European Research Center, \\ 54622, Thessaloniki, Greece\\
  \email{{jmalvarez@seerc.org}}\\
\and WESO Research Group, Department of Computer Science, University of Oviedo, \\ 33007, Oviedo, Spain.\\
  \email{{labra@uniovi.es.es}}\\  
  \and WESO Research Group, Department of Business Administration, University of Oviedo, \\ 33007, Oviedo, Spain.\\
  \email{{patriop@uniovi.es.es}}\\  
}


\date{}

\maketitle

\renewcommand{\labelitemi}{$\bullet$}

\begin{abstract}
FIXME
\end{abstract}

\section{Introduction}
Public and private bodies are continuously seeking for new analytical tools and methods to assess, rank and compare their performance based 
on different indicators and dimensions with the objective of making some decision or developing a new policy. 
In this context the creation and use of quantitative indexes is a wider but critized practice that has been applied to various 
domains such as bibliometrics and academic performance and quality (the Impact Factor by Thomson-Reuters or the H-index), 
the Web impact (the Webindex by the Webfoundation) or Cloud Computing (the CSMIC index, the Global Cloud Index by Cisco, the CSC index, 
the VMWare Cloud Index, etc.) or university quality (the Shanghai or the Webometrics rankings) to name a few. 
Therefore policy-makers as well as individuals are continuously evaluating quantitative measures to tackle 
existing problems or support their decisions. Nevertheless the sheer mass of data now available in the web is 
raising a new dynamic and challenging environment in which traditional tools are facing major 
problems to deal with datasources diversity, structural issues or complex processes of estimation. Following some efforts 
such as the ``Policy-making $2.0$'' within the Cross-Over project that \textit{refers to a blend of emerging and fast developing technologies 
that enable better, more timely and more participated decision-making}, new paradigms and tools are required to take advantage of 
the existing environment (open data and big data) to design and estimate this dynamic environment according to new requirements of 
transparency, standardization, adaptability and extensibility among others with the aim of providing new context-aware 
and added-value services such as visualization that can help a deepen and broaden understanding of the impact of a 
policy in a more fast and efficient way. As a consequence common features and requirements can be extracted out from the existing situation:
\begin{itemize}
 \item Data sources. Data and information is continuously being generated as observations from social networks, public and private institutions, NGOs, services and applications, etc. 
 creating a tangled environment of sources, formats and access protocols with a huge potential for exploitation. Nevertheless data processing, knowledge inferring, etc. are not mere processes 
 of retrieving and gathering, it is necessary to deal with semantic and syntactic issues with the aim of enabling a proper re-use of data and information 
 and generate new knowledge. 
 
 \item Structure. Quantitative indexes are usually defined (a mathematical model) by experts to aggregate several indicators (in a tree structure) in just one value providing 
 a measure of the impact or performance of some policy in a certain context. The structure of these indexes are obviously subjected to change over time 
 to collect more information or adjust some parameters. That is why technology should be able to afford the proper techniques 
 to automatically represent new changes in an efficient way.
 
  \item Computation process. This feature refers to the calculation of the index. Observations are gathered from different data sources and aligned 
  to the index structure, commonly indicators, that are processed through different mathematical operators to compute the final value of the index. 
  Nevertheless the computation process is not always open and can not be replied for third-parties with other purposes, for instance research, preventing some 
  of most wanted characteristics such as transparency. Furhermore it is not possible to assess if the index is going to be or has been computed in 
  the right way.
  
  \item Documentation. As the European project Cross-over has stated, new policy-making strategies go ahead of a simple and closed value and it is necessary to provide 
  new ways of exploiting data and information. Moreover the use of the Web as a dissemination channel represents a powerful environment in which 
  information should be available taking into account multilingual and multicultural character of information. In this context documentation mechanisms 
  must necessarily cover all the aforementioned features to afford a detailed explaination of a quantitative index-based policy to both policy-makers 
  and final users but...
\end{itemize}

On the other hand, the Semantic Web area has experienced during last years a growing commitment from both academia and industrial areas 
with the objective of elevating the meaning of web information resources through a common and shared data model (graphs) and 
an underlying semantics based on different logic formalisms (ontologies). The Resource Description Framework (RDF), based on a graph model, 
and the Web Ontology Language (OWL), designed to formalize and model domain knowledge, are a \textit{lingua-franca} to reuse information 
and data in a knowledge-based environment. Thus data, information and knowledge can be easily shared, exchanged and linked~\cite{Maali_Cyganiak_2011} 
to other knowledge and databases through the use URIs, more specifically HTTP-URIs. Therefore the broad objective of this effort can 
be summarized as a new environment of added-value services that can encourage and improve B2B (Business to Business), B2C (Business to Client) or 
A2A (Administration to Administration) relationships by means of the implementation of new contex-awareness expert systems 
to tackle existing cross-domain problems in which data heterogeneities, lack of standard knowledge representation and 
interoperability problems are common factors. As part of the Semantic Web area, recent times have also seen the deployment of 
the Linked (Open) Data initiative  to make it possible the view and application of the Semantic Web to create a large 
and distributed database on the Web. 

Obviously semantic web technologies and tools can partially fulfill the features and requirements of this challenging environment for supporting 
new policy-making strategies. A common and shared data model based on existing standardized semantic web vocabularies and datasets can be used to 
represent quantitative indexes from both, structural and computational, points of view enabling the right exploitation of metadata and semantics. 
That is why the present paper introduces: 1) a high-level model on top of the RDF Data Cube Vocabulary, a shared effort to model statistical data in RDF reusing parts 
(the cube model) of the Statistical Data and Metadata Exchange Vocabulary (SDMX), to represent the structure of quantitative indexes and 
2) a Java-SPARQL based processor to exploit the metainformation and compute new values of an index.

Finally, as a motivating example, see Table~\ref{tab:example-wb}, a policy-maker wants to re-use the WorldBank data to model and compute a new index, ``The Naive World Bank Index''. This 
index uses the topics ``Aid Efectiveness'' ($c_1$) and ``Health'' ($c_2$) with the aim of comparing the status and evolution of health in several countries to decide whether new 
investments are performed. From these components two indicators has been respectively selected by experts: ``Life Expectancy'' ($i_1$) and ``Health expenditure, total (\%) of GDP'' ($i_2$). 
Once components and their indicators are defined and data can be retrieved from the WorldBank it is necessary to set how the index and components are computed 
taking into account that only data about indicators is available. Following a top-down approach the index, $i$, is calculated through ordered weighted averaging (OWA) using the 
formula: $\sum_{i=1}^n  w_i c_i$, where $w_i$ is the weight of the component $c_i$. On the other hand, both components only aggregates one indicator but the ``Aid Efectiveness'' 
must firstly compute the ``Life Expectancy'' without considering the sex dimension. Making this implies the need of calculating the average age by country and year to create 
a new ``derivated'' indicator of ``Life Expectancy''. Apart from that the computation must also consider the observation status and values must be normalized using the 
z-score before computing intermediate and final values for each indicator, component and index. Furthermore this index is supposed to change in the future 
adding new data sources, modifying the computation processes (weights) or the structure (new components, indicators and dimensions). Finally, the policy-maker 
is also interested in applying this index to other scenarios and he needs a way of explaining in different languages how the index is computed. 


\begin{table}[!htb]
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|p{3cm}||p{5cm}|p{1.8cm}|p{1.8cm}|p{1cm}|}
\hline
  \textbf{Component} & \textbf{Indicator} & \textbf{Year} & \textbf{Country} & \textbf{Value}  \\  \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Greece & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2011 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2011 & Spain & $1.0$ \\ \hline
  \hline
  \end{tabular}
  \caption{Example of observations from the WorldBank.}
  \label{tab:example-wb}
  \end{center}
\end{table} 


\section{Related Work}
The present work is focused in applying semantic web vocabularies and datasets to model 
quantitative indexes from both structural and computational points of view. Currently one 
of the mainstreams in the Semantic Web area is the Linked Data initiative which principles 
have been applied to different domains such as  e-Government, e-Health, Biomedicine, Education, Bibliography or Geography 
to name a few,  with the aim of solving existing problems of integration and interoperability 
among applications and create a proper knowledge environment under Web-based protocolos. 

In order to reach this major objective the publication of information and data under a common data model (RDF) and formats with 
a specific formal query language (SPARQL~\cite{Sparql11}) provide the required building blocks to turn the Web of documents 
into a real database of data~\cite{freebase}. As a consequence the popular diagram of the Linked Data Cloud~\cite{linked-data-cloud}, 
generated from metadata extracted from the Comprehensive Knowledge Archive Network (CKAN~\cite{ckan}) out, 
contains $337$ datasets, with more than $25$ billion RDF triples and $395$ million links in different  domains. 
Research works are focused in two main areas: 1) production/publishing~\cite{bizer07how} and 2) consumption of  Linked Data. 
In the first case data quality~\cite{bizer2007,wiqa,ld-quality,lodq,link-qa}, conformance~\cite{HoganUHCPD:2012:237}, 
provenance~\cite{w3c-prov,DBLP:conf/ipaw/HartigZ10}, trust~\cite{Carroll05namedgraphs}, description of datasets~\cite{void,Cyganiak08semanticsitemaps,ckanValidator} and 
entity reconciliation~\cite{Serimi,Maali_Cyganiak_2011} are becoming major objectives since a mass of amount data is already available~\cite{Triplify} 
through SPARQL endpoints deployed on the top of RDF repositories such as OpenLink Virtuoso or OWLim. 

On the other hand, consumption of Linked Data is being addressed to provide new ways of data visualization~\cite{DBLP:journals/semweb/DadzieR11,hoga-etal-2011-swse-JWS}, 
faceted browsing~\cite{Pietriga06fresnel,citeulike:8529753,Sparallax} and searching~\cite{hoga-etal-2011-swse-JWS}, processing~\cite{Harth:2011:SIP:1963192.1963318} and exploitation of data applying 
different approaches such as sensors~\cite{Jeung:2010:EMM:1850003.1850235,ontology-search} and techniques  such as distributed 
queries\cite{Hartig09executingsparql,Ankolekar07thetwo,sparqlOpt}, scalable reasoning process~\cite{DBLP:conf/semweb/UrbaniKOH09,HoganHarthPolleres2009,DBLP:conf/semweb/HoganPPD10}, 
annnotation of web pages~\cite{rdfa-primer} or information retrieval~\cite{Pound} to name a few.

In the particular case of statistical data, the RDF Data Cube Vocabulary~\cite{rdf-data-cube}
a W3C Working Draft document, is a shared effort to represent statistical data in RDF reusing parts (the cube model) 
of the Statistical Data and Metadata Exchange Vocabulary (SDMX)~\cite{sdmx}, an ISO standard 
for exchanging and sharing statistical data and metadata among organizations. The Data Cube vocabulary is a core 
foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or 
other multi-dimensional data sets. Previously, the Statistical Core Vocabulary~\cite{scovo} was the standard in fact to describe statistical information in the Web of Data.
Some works are also emerging to publish statistical data following the concepts of the LOD initiative 
such as~\cite{DBLP:conf/semweb/ZapilkoM11,DBLP:journals/ijsc/SalasMBCMA12,DDI2013,DBLP:conf/dgo/FernandezMG11} among others.

\section{Theoretical modelling of a quantitative composite index}
This section outlines a model for representing quantitative indexes based on the aggregation 
of different components and indicators. Furthermore a computation process for those elements is 
also presented in order to specify the calculation of new observations.

Basically, a quantitative index is comprised of the aggregation of several component observations. In the same way, 
a component is also composed of the aggreagation of indicators that keep real observations. From this initial definition 
some characteristics and assumptions can be found: 1) although observations can be directly mapped to an index or a component, they 
are usually computed applying a bottom-up approach from an indicator to a component and index. 2) An observation is 
a real numerical value extracted from some agent out under a certain context. Generally observations only takes one measure and are considered 
to be raw without any pre-processing technique. 3) Before aggregating observation values, componens and indexes can 
estimate missing values to finally normalize them in order to get a final quantitative value.

According to the aforementioned characteristics and assumptions an ``observable'' element (index, component or indicator) is a 
dataset of numerical observations under a specific context (dimensions and/or metadata) that can be directly extracted from external 
sources out or computed by some kind of OWA operator. 

\begin{definition}[Observation-$o$]\upshape
It is a tuple $\{v,m,s\}$, where $v$ is a numerical value for the measure $m$ with an status $s$ that belongs to 
only one dataset of observations $O$. 
\end{definition}


\begin{definition}[Dataset-$q$]\upshape
It is the tuple $\{O,m,D,A,T\}$ where $O$ is a set of observations for only one measure $m$ that is described under 
a set of dimensions $D$ and a set of annotations $A$. Additionally, some attributes can be defined in the set $T$ for structure enrichment. 
\end{definition}


\begin{definition}[Aggregated dataset-$aq$]\upshape
It is a composed dataset which set of observations $O$ is derivated by applying 
an OWA operator $p$ to the observations $O_i$ of each aggregated dataset $Q_i$.
\end{definition}

As a necessary condition for the computation process, an aggregated dataset $aq$ defined by means of the set of dimensions $D_{aq}$ can be computed iif 
$\forall q_j \in Q_i: D_{aq} \subseteq D_{q_j}$. Furthermore the OWA operator $p$ can only aggregate values belonging to the same measure $m$.


As a consequence of the aforementioned definitions some remarks must be outlined in order to restrict the understanding of 
a quantitative index (structure and computation):
\begin{itemize}
 \item The set of dimensions $D$, annotations $A$ and $T$ for a given dataset $Q$ is always the same with the aim of describing all observations under 
 the same context.
 \item An index $i$ and a component $c$ are aggregated datasets. Neverthless this restriction is relaxed if observations can be directly mapped to 
 these elements without any computation processes.
 \item An indicator $in$ can be both dataset or aggregated dataset.
 \item All elements in definitions must be uniquely identified. 
\end{itemize}

Following the on-going example, see Table~\ref{tab:example-wb}, the modelling of the ``The Naive World Bank Index'' would be the next one:
\begin{itemize}
 \item Each row of the table is an observation $o_i$ with a numerical value $v$, the measure is $m_{in}$ and the status is ``Raw''.
 \item Two indicators can be found: \{ ($in_1$, ``Life Expectancy''), ($in_2$, ``Health expenditure, total (\% of GDP)'') \}, each indicator contains a set 
 of observations $O_{in_i}$. The dimensions for each indicator are: $D_{in_1}$  \{(``Year'', ``Country'', ``Sex''\} and $D_{in_2}$ \{``Year'', ``Country''\}.
 \item In order to group the ``Life Expectancy'' without the ``Sex'' dimension it is necessary to define a new aggregated dataset $aq_1$ which 
 dimensions $D_{aq_1}$ are \{``Year'', ``Country''\} and the OWA operator is the average of the values $v$. In this sample the aggregated indicator $aq_1$
 can be built due to the indicator ``Life Expectancy'' accomplish with the aforementioned necessary conditions: 1) $D_{aq} \subseteq D_{in_1} \wedge D_{aq_1} \subseteq D_{in_2}$ and 
 2) $m_{aq_1}= m_{in_1}$ = ``Life Expectancy''.
 \item In the same way, the set of components: \{($c_1$,``Aid Efectiveness''), ($c_2$,``Health'')\} are built aggregating the indicators $aq_1$ and 
 $q_2$ using as OWA operator the ``min'' value. In this case ``min'' or ``max'' operators can be used due to an observation is uniquely identified in a 
 dataset by a tuple $\{v,m,s\} \cup D$.
 \item Finally, the index is computed using the general form of an OWA operator $\sum_{i=1}^n  w_i c_i$ and taking as weights those we select.
\end{itemize}

As final remark, the computation process is generating new observations, following a bottom-up approach, according to the structure defined 
in each dataset. Although a logical structure of indexes, components and indicators can be directly established using narrower/broader properties 
the main advantage lies in the possibility of expressing new elements by aggregating others describing their structure. Nevertheless restrictions 
about the type of dataset that can be aggregated in each level could be added at any time for other reseasons such as validation or to generate 
a human-readble form of the index.


\section{Representation of a quantitative composite index in RDF: The RDFIndex}
Since previous section has stated the building blocks to represent quantitative indexes by aggregation a direct translation built 
on top of the RDF Data Cube Vocabulary, SDMX and other semantic web vocabularies is presented in Table~\ref{index-to-rdf}. Thus 
all concepts in the index are described reusing existing definitions, taking advantage of previous efforts and pre-established semantics 
with the aim of being extended in the future to fit new requirements.

\begin{table}[!htb]
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|p{3cm}||p{6cm}|p{3cm}|}
\hline
  \textbf{Concept} & \textbf{Vocabulary element} &  \textbf{Comments}  \\  \hline
   Observation $o$ & \texttt{qb:observation} &  \\ \hline
   Numerical value $v$ & \texttt{xsd:double} &  \\ \hline
   Measure $m$ & \texttt{qb:MeasureProperty} \texttt{sdmx-measure:obsValue} &  \\ \hline
   Status $s$ & \texttt{sdmx-concept:obsStatus} &  \\ \hline
   Dataset $q$ & \texttt{qb:dataSet} &  \\ \hline
   Dimension $d_i \in D$ & \texttt{qb:DimensionProperty} &  \\ \hline
   Annotation $a_i \in A$ & \texttt{owl:AnnotationProperty} &  \\ \hline
   Attribute $at_i \in T$ & \texttt{qb:AttributeProperty} &  \\ \hline
   OWA operator $p$ &  \texttt{skos:Concept} and SPARQL 1.1 aggregation operators & FIXME \\ \hline
   Index, Component and Indicator & \texttt{skos:Concept} & FIXME \\ \hline
  \hline
  \end{tabular}
  \caption{Mapping between the index definition and the RDF Data Cube Vocabulary.}
  \label{index-to-rdf}
  \end{center}
\end{table} 

FIXME: example



\section{A $\{Java\rightarrow based \rightarrow SPARQL\}$  interpreter of the RDFIndex}
%
\section{Use Case: FIXME}
% % 
\section{Discussion and Further Steps}

\section{Conclusions and Future Work}

\bibliographystyle{plain}
% %\bibliographystyle{unsrt}
% %\bibliographystyle{acm}
\bibliography{references}
% \renewcommand{\bibname}{References}

\end{document}

