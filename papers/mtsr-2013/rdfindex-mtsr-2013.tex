\documentclass{llncs}

%\usepackage{llncsdoc}

%\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multirow}

\usepackage{url}
\usepackage{rotating}

%%%Math
\usepackage{latexsym}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{amsthm}
%\usepackage{eurosans}

\usepackage{eurosym}

\usepackage{longtable}

\usepackage{listings}

\usepackage{color}
\usepackage{textcomp}


\definecolor{gray}{gray}{0.5}
\definecolor{green}{rgb}{0,0.5,0}


\begin{document}
\title{Leveraging semantics to represent and compute quantitative indexes. \\ The RDFindex approach.}

\author{Jose Mar\'{i}a \'{A}lvarez\inst{1} \and Jos\'{e} Emilio Labra\inst{2}  \and Patricia Ordoñez de Pablos\inst{3}}


\authorrunning{Jose Mar\'{i}a Alvarez \and  Jos\'{e} Emilio Labra \and Patricia Ordoñez de Pablos}


\institute{South East European Research Center, \\ 54622, Thessaloniki, Greece\\
  \email{{jmalvarez@seerc.org}}\\
\and WESO Research Group, Department of Computer Science, University of Oviedo, \\ 33007, Oviedo, Spain.\\
  \email{{labra@uniovi.es.es}}\\  
  \and WESO Research Group, Department of Business Administration, University of Oviedo, \\ 33007, Oviedo, Spain.\\
  \email{{patriop@uniovi.es.es}}\\  
}


\date{}

\maketitle

\renewcommand{\labelitemi}{$\bullet$}

\begin{abstract}
FIXME
\end{abstract}

\section{Introduction}
Public and private bodies are continuously seeking for new analytical tools and methods to assess, rank and compare their performance based 
on different indicators and dimensions with the objective of making some decision or developing a new policy. 
In this context the creation and use of quantitative indexes is a wider but critized practice that has been applied to various 
domains such as bibliometrics and academic performance and quality (the Impact Factor by Thomson-Reuters or the H-index), 
the Web impact (the Webindex by the Webfoundation) or Cloud Computing (the CSMIC index, the Global Cloud Index by Cisco, the CSC index, 
the VMWare Cloud Index, etc.) or university quality (the Shanghai or the Webometrics rankings) to name a few. 
Therefore policy-makers as well as individuals are continuously evaluating quantitative measures to tackle 
existing problems or support their decisions. Nevertheless the sheer mass of data now available in the web is 
raising a new dynamic and challenging environment in which traditional tools are facing major 
problems to deal with datasources diversity, structural issues or complex processes of estimation. Following some efforts 
such as the ``Policy-making $2.0$'' within the Cross-Over project that \textit{refers to a blend of emerging and fast developing technologies 
that enable better, more timely and more participated decision-making}, new paradigms and tools are required to take advantage of 
the existing environment (open data and big data) to design and estimate this dynamic environment according to new requirements of 
transparency, standardization, adaptability and extensibility among others with the aim of providing new context-aware 
and added-value services such as visualization that can help a deepen and broaden understanding of the impact of a 
policy in a more fast and efficient way. As a consequence common features and requirements can be extracted out from the existing situation:
\begin{itemize}
 \item Data sources. Data and information is continuously being generated as observations from social networks, public and private institutions, NGOs, services and applications, etc. 
 creating  a \textit{marem\'{a}gnum} of sources, formats and access protocols with a huge potential for exploitation. Nevertheless data processing, knowledge inferring, etc. are not mere processes 
 of retrieving and gathering, it is necessary to deal with semantic and syntactic issues with the aim of enabling a proper re-use of data and information 
 and generate new knowledge. 
 
 \item Structure. Quantitative indexes are usually defined (a mathematical model) by experts to aggregate several indicators (in a tree structure) in just one value providing 
 a measure of the impact or performance of some policy in a certain context. The structure of these indexes are obviously subjected to change over time 
 to collect more information or adjust some parameters. That is why technology should be able to afford the proper techniques 
 to automatically represent new changes in an efficient way.
 
  \item Computation process. This feature refers to the calculation of the index. Observations are gathered from different data sources and aligned 
  to the index structure, commonly indicators, that are processed through different mathematical operators to compute the final value of the index. 
  Nevertheless the computation process is not always open and can not be replied for third-parties with other purposes, for instance research, preventing some 
  of most wanted characteristics such as transparency. Furhermore it is not possible to assess if the index is going to be or has been computed in 
  the right way.
  
  \item Documentation. As the European project Cross-over has stated, new policy-making strategies go ahead of a simple and closed value and it is necessary to provide 
  new ways of exploiting data and information. Moreover the use of the Web as a dissemination channel represents a powerful environment in which 
  information should be available taking into account multilingual and multicultural character of information. In this context documentation mechanisms 
  must necessarily cover all the aforementioned features to afford a detailed explaination of a quantitative index-based policy to both policy-makers 
  and final users but...
\end{itemize}

On the other hand, the Semantic Web area has experienced during last years a growing commitment from both academia and industrial areas 
with the objective of elevating the meaning of web information resources through a common and shared data model (graphs) and 
an underlying semantics based on different logic formalisms (ontologies). The Resource Description Framework (RDF), based on a graph model, 
and the Web Ontology Language (OWL), designed to formalize and model domain knowledge, are a \textit{lingua-franca} to reuse information 
and data in a knowledge-based environment. Thus data, information and knowledge can be easily shared, exchanged and linked~\cite{Maali_Cyganiak_2011} 
to other knowledge and databases through the use URIs, more specifically HTTP-URIs. Therefore the broad objective of this effort can 
be summarized as a new environment of added-value services that can encourage and improve B2B (Business to Business), B2C (Business to Client) or 
A2A (Administration to Administration) relationships by means of the implementation of new contex-awareness expert systems 
to tackle existing cross-domain problems in which data heterogeneities, lack of standard knowledge representation and 
interoperability problems are common factors. As part of the Semantic Web area, recent times have also seen the deployment of 
the Linked (Open) Data initiative  to make it possible the view and application of the Semantic Web to create a large 
and distributed database on the Web. 

Obviously semantic web technologies and tools can partially fulfill the features and requirements of this challenging environment for supporting 
new policy-making strategies. A common and shared data model based on existing standardized semantic web vocabularies and datasets can be used to 
represent quantitative indexes from both, structural and computational, points of view enabling the right exploitation of metadata and semantics. 
That is why the present paper introduces: 1) a high-level model on top of the RDF Data Cube Vocabulary, a shared effort to model statistical data in RDF reusing parts 
(the cube model) of the Statistical Data and Metadata Exchange Vocabulary (SDMX), to represent the structure of quantitative indexes and 
2) a Java-SPARQL based processor to exploit the metainformation and compute new values of an index.

Finally, as a motivating example, see Table~\ref{tab:example-wb}, a policy-maker wants to re-use the WorldBank data to model and compute a new index, ``The Naive World Bank Index''. This 
index uses the topics ``Aid Efectiveness'' ($c_1$) and ``Health'' ($c_2$) with the aim of comparing the status and evolution of health in several countries to decide whether new 
investments are performed. From these components two indicators has been respectively selected by experts: ``Life Expectancy'' ($i_1$) and ``Health expenditure, total (\%) of GDP'' ($i_2$). 
Once components and their indicators are defined and data can be retrieved from the WorldBank it is necessary to set how the index and components are computed 
taking into account that only data about indicators is available. Following a top-down approach the index, $i$, is calculated through ordered weighted averaging (OWA) using the 
formula: $\sum_{i=1}^n  w_i c_i$, where $w_i$ is the weight of the component $c_i$. On the other hand, both components only aggregates one indicator but the ``Aid Efectiveness'' 
must firstly compute the ``Life Expectancy'' without considering the sex dimension. Making this implies the need of calculating the average age by country and year to create 
a new ``derivated'' indicator of ``Life Expectancy''. Apart from that the computation must also consider the observation status and values must be normalized using the 
z-score before computing intermediate and final values for each indicator, component and index. Furthermore this index is supposed to change in the future 
adding new data sources, modifying the computation processes (weights) or the structure (new components, indicators and dimensions). Finally, the policy-maker 
is also interested in applying this index to other scenarios and he needs a way of explaining in different languages how the index is computed. 


\begin{table}[!htb]
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|p{3cm}||p{5cm}|p{1.8cm}|p{1.8cm}|p{1cm}|}
\hline
  \textbf{Component} & \textbf{Indicator} & \textbf{Year} & \textbf{Country} & \textbf{Value}  \\  \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Greece & $1.0$ \\ \hline
  Health & Health expenditure, total (\%) of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\%) of GDP) & 2011 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\%) of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\%) of GDP) & 2011 & Spain & $1.0$ \\ \hline
  \hline
  \end{tabular}
  \caption{Example of observations from the WorldBank.}
  \label{tab:example-wb}
  \end{center}
\end{table} 


\section{Related Work}
The present work is focused in applying Semantic Web and Linked Data principles to model 
quantitative indexes from both structural and computational points of view. 

% 
% In the same way the 
% effort carried out by international standardization organisms such as the W3C has produced and is currently delivering 
% new specifications, vocabularies and models to boost the use of semantic technologies in a broad scope 
% of domains such as e-Government, e-Health, Biomedicine, Education, Bibliography or Geography to name a few, 
% with the aim of solving existing problems of integration and interoperability among applications and create 
% a proper knowledge environment under Web-based protocolos.
% 
% In order to reach this major objective the publication of information 
% and data under a common data model (RDF) and formats with a specific formal query language (SPARQL~\cite{Sparql11}) provide the required building blocks to turn the Web of documents 
% into a real database of data~\cite{freebase}. As a consequence the popular diagram of the Linked Data Cloud~\cite{linked-data-cloud}, 
% generated from metadata extracted from the Comprehensive Knowledge Archive Network (CKAN~\cite{ckan}) out, 
% contains $337$ datasets, with more than $25$ billion RDF triples and $395$ million links in different  domains. 
% Research works are focused in two main areas: 1) production/publishing~\cite{bizer07how} and 2) consumption of  Linked Data. 
% In the first case data quality~\cite{bizer2007,wiqa,ld-quality,DBLP:journals/ws/BizerC09,lodq,link-qa}, conformance~\cite{DBLP:journals/ws/HoganUHCPD12}, 
% provenance~\cite{w3c-prov,DBLP:conf/ipaw/HartigZ10}, trust~\cite{Carroll05namedgraphs}, description of datasets~\cite{void,Cyganiak08semanticsitemaps,ckanValidator} and 
% entity reconciliation~\cite{Serimi,Maali_Cyganiak_2011} are becoming major objectives since a mass of amount data is already available~\cite{Triplify} 
% through SPARQL endpoints deployed on the top of RDF repositories such as OpenLink Virtuoso or OWLim. 
% On the other hand, consumption of Linked Data is being addressed to provide new ways of data visualization~\cite{DBLP:journals/semweb/DadzieR11,hoga-etal-2011-swse-JWS}, 
% faceted browsing~\cite{Pietriga06fresnel, citeulike:8529753,Sparallax} and searching~\cite{hoga-etal-2011-swse-JWS}, processing~\cite{Harth:2011:SIP:1963192.1963318} and exploitation of data applying 
% different approaches such as sensors~\cite{Jeung:2010:EMM:1850003.1850235,ontology-search} and techniques  such as distributed 
% queries\cite{Hartig09executingsparql,Ankolekar07thetwo,sparqlOpt}, scalable reasoning process~\cite{Urbani2010WebPIE,HoganHarthPolleres2009,DBLP:conf/semweb/HoganPPD10}, 
% annnotation of web pages~\cite{rdfa-primer} or information retrieval~\cite{Pound} to name a few.



% In the particular case of statistical data, the RDF Data Cube Vocabulary (Cyganiak & Reynolds, 2012), 
% a W3C Working Draft document, is a shared effort to represent statistical data in RDF reusing parts (the cube model) 
% of the Statistical Data and Metadata Exchange Vocabulary (SDMX) (SDMX initiative, 2005), an ISO standard 
% for exchanging and sharing statistical data and metadata among organizations. The Data Cube vocabulary is a core 
% foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or 
% other multi-dimensional data sets. Previously, the Statistical Core Vocabulary (SCOVO, 2009), published by DERI,
% was the standard in fact to describe statistical information in the Web of Data. Some works are also emerging 
% to publish statistical data following the concepts of the LOD initiative such as (Bosch, Cyganiak, Wackerow et al., 2012), (Zapilko & Mathiak, 2012) or 
% (Rivera-Salas, Martin,  Maia Da Mota,  Auer, S. et al., 2012) among others.


\section{Theoretical modelling of a quantitative composite index}
\section{Representation of a quantitative composite index in RDF: The RDFIndex}
\section{A $\{Java\rightarrow based \rightarrow SPARQL\}$  interpreter of the RDFIndex}
%
\section{Use Case: FIXME}
% % 
\section{Discussion and Further Steps}

\section{Conclusions and Future Work}

\bibliographystyle{plain}
% %\bibliographystyle{unsrt}
% %\bibliographystyle{acm}
\bibliography{references}
% \renewcommand{\bibname}{References}

\end{document}

